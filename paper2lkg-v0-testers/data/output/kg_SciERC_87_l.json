{
  "iri": "Paper-87",
  "title": "N03-1033",
  "authors": [],
  "keywords": [],
  "sections": [
    {
      "iri": "Paper-87-Section-1",
      "subtitle": "Abstract",
      "paragraphs": [
        {
          "iri": "Paper-87-Section-1-Paragraph-1",
          "sentences": [
            {
              "iri": "Paper-87-Section-1-Paragraph-1-Sentence-1",
              "text": "We present a new part-of-speech tagger that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features ."
            },
            {
              "iri": "Paper-87-Section-1-Paragraph-1-Sentence-2",
              "text": "Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result ."
            }
          ]
        }
      ]
    }
  ],
  "summary": "We present a new part-of-speech tagger that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features . Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result .",
  "kg2text": [
    "A novel part-of-speech tagger, referred to as 'the resulting tagger', has a broader term of both 'tagger' and 'part-of-speech tagger'. This new tagger achieves an impressive accuracy rate of 97.24% on the Penn Treebank WSJ dataset. Moreover, it demonstrates several innovative ideas in natural language processing."
  ],
  "times": [
    3.8696863651275635
  ]
}